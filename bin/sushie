#!/usr/bin/env python

from __future__ import division
import argparse
import bz2
import gzip
import logging
import os
import re
import sys
import warnings
import copy
import limix.her as her

import numpy as np
import pandas as pd

from scipy.stats import chi2
from sqlalchemy import exc as sa_exc

from pandas_plink import read_plink
import scipy.linalg as linalg
from scipy import stats
from statsmodels.stats.multitest import multipletests
from jax.config import config
import math
from jax import random
import statsmodels.api as sm
import sushie

warnings.filterwarnings('ignore')
with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    import jax.numpy as jnp

np.seterr(invalid='ignore')

def get_command_string(args):
    """
    Format focus call and options into a string for logging/printing
    :return: string containing formatted arguments to focus
    """

    base = "focus {}{}".format(args[0], os.linesep)
    rest = args[1:]
    rest_strs = []
    needs_tab = True
    for cmd in rest:
        if "-" == cmd[0]:
            if cmd in ["--quiet", "-q", "--verbose", "-v"]:
                rest_strs.append("\t{}{}".format(cmd, os.linesep))
                needs_tab = True
            else:
                rest_strs.append("\t{}".format(cmd))
                needs_tab = False
        else:
            if needs_tab:
                rest_strs.append("\t{}{}".format(cmd, os.linesep))
                needs_tab = True
            else:
                rest_strs.append(" {}{}".format(cmd, os.linesep))
                needs_tab = True

    return base + "".join(rest_strs) + os.linesep

def drop_nainf(df, nam, idx):
    log = logging.getLogger(sushie.LOG)
    old_row = df.shape[0]
    df.replace([jnp.inf, -jnp.inf], jnp.nan)
    df.dropna(inplace=True)
    diff =  old_row - df.shape[0]
    if diff != 0:
        log.info(f"Ancestry {idx+1}: Drop {diff} rows from {nam} table due to INF value or NAN value.")
    return df

def regress_resid(X, y):
    """
    Perform a marginal linear regression for each snp on the phenotype.

    :param X: numpy.ndarray n x p genotype matrix to regress over
    :param y: numpy.ndarray phenotype vector

    :return: pandas.DataFrame containing estimated beta and standard error
    """

    tmp_X = sm.add_constant(X)
    result = sm.OLS(y, tmp_X).fit()

    return result.resid

def allele_check(base0, base1, comp0, comp1):
    """
    Check whether SNPs alleles match across ancestries.

    :param base0: A0 for the first ancestry (baseline)
    :param base1: A1 for the first ancestry (baseline)
    :param comp0: A0 for the compared ancestry
    :param comp1: A1 for the compared ancestry

    :return: pandas.DataFrame containing estimated beta and standard error
    """

    log = logging.getLogger(sushie.LOG)
    base0 = base0.astype("object")
    base1 = base1.astype("object")
    comp0 = comp0.astype("object")
    comp1 = comp1.astype("object")
    correct_idx = jnp.where(jnp.logical_and(base0 == comp0, base1 == comp1))[0]
    flipped_idx = jnp.where(jnp.logical_and(base0 == comp1, base1 == comp0))[0]
    wrong_idx = jnp.where(jnp.logical_not(jnp.logical_or(jnp.logical_and(base0 == comp0, base1 == comp1),
                                                        jnp.logical_and(base0 == comp1, base1 == comp0))))[0]

    return correct_idx, flipped_idx, wrong_idx

def estimate_her(X, y, C):
    n, p = X.shape
    A = jnp.dot(X, X.T) / p
    h2g = her.estimate(y, "normal", A, C, verbose=False)

    return h2g

def process_raw(geno_paths, pheno_paths, covar_paths, L, resid):
    log = logging.getLogger(sushie.LOG)

    n_pop = len(geno_paths)

    bim = []
    fam = []
    bed = []
    pheno = []

    for idx in range(n_pop):
        log.info(f"Ancestry {idx+1}: Reading in genotype data and phenotype data.")
        tmp_bim, tmp_fam, tmp_bed = read_plink(geno_paths[idx], verbose=False)

        # read pheno data
        tmp_pheno = pd.read_csv(pheno_paths[idx], sep = "\t", header = None, dtype={0: object})

        # drop all the nan inf values
        tmp_bim = drop_nainf(tmp_bim, "bim", idx)
        tmp_fam = drop_nainf(tmp_fam, "fam", idx)
        tmp_pheno = drop_nainf(tmp_pheno, "pheno", idx)
        # rename the columns with pop index for better processing in the future
        tmp_bim = tmp_bim[["chrom", "snp", "a0", "a1", "cm", "pos", "i"]].rename(columns = {"i":f"bimIDX_{idx+1}",
                                            "a0":f"a0_{idx+1}",
                                            "a1":f"a1_{idx+1}",
                                            "cm": f"cm_{idx+1}",
                                            "pos": f"pos_{idx+1}"})
        tmp_fam = tmp_fam[["fid", "iid", "i"]].rename(columns = {"i":f"famIDX_{idx+1}"})
        tmp_pheno.reset_index(inplace=True)
        # keep track of pheno index for future mathcing the bed file if bed files are shuffled due to merging
        tmp_pheno = tmp_pheno.rename(columns={"index": f"phenoIDX_{idx+1}", 0:"fid", 1:"iid"})

        if len(tmp_bim) == 0:
            raise ValueError(f"Ancestry {idx+1}: No genotype data found for ancestry at {df_paths[idx]}.")

        tmp_bed = tmp_bed.compute()
        bim.append(tmp_bim)
        fam.append(tmp_fam)
        bed.append(tmp_bed)
        pheno.append(tmp_pheno)

    # read in covar
    if covar_paths is not None:
        covar = []
        for idx in range(n_pop):
            tmp_covar = pd.read_csv(covar_paths[idx], sep = "\t", header = None, dtype={0: object})
            tmp_covar = drop_nainf(tmp_covar, "covar", idx)
            tmp_covar.reset_index(inplace=True)
            # keep track of covar index for future mathcing the bed file if bed files are shuffled due to merging
            tmp_covar = tmp_covar.rename(columns={"index": f"covarIDX_{idx+1}", 0:"fid", 1:"iid"})
            covar.append(tmp_covar)
    else:
        covar = [None] * n_pop
        log.warning(f"No covariates detected for this analysis.")

    # find common snps across ancestries
    if n_pop > 1:
        snps = pd.merge(bim[0], bim[1], how="inner", on=["chrom", "snp"])
        for idx in range(n_pop - 2):
            snps = pd.merge(snps, bim[idx+2], how="inner", on=["chrom", "snp"])
        # report how many snps we removed due to indep SNP
        for idx in range(n_pop):
            snps_num_diff = bim[idx].shape[0] - snps.shape[0]
            log.warning(f"Ancestry{idx+1} has {snps_num_diff} independent SNPs and {snps.shape[0]} common SNPs. Will remove these independent SNPs.")
    else:
        snps = bim[0]

    if snps.shape[0] < L:
        raise ValueError(f"The number of common SNPs across ancestries is less than inferred L. Please choose a smaller L or expand the genomic window.")

    # find filped reference alleles across ancestires
    flip_idx = []
    if n_pop > 1:
        for idx in range(1, n_pop):
            correct_idx, tmp_flip_idx, wrong_idx = allele_check(snps["a0_1"].values, snps["a1_1"].values, snps[f"a0_{idx+1}"].values, snps[f"a1_{idx+1}"].values)

            # save the index for future swapping
            flip_idx.append(tmp_flip_idx)
            log.warning(f"Ancestry{idx+1} has {tmp_flip_idx.shape[0]} flipped alleles from ancestry 1. Will flip these SNPs.")
            if wrong_idx.shape[0] != 0:
                snps = snps.drop(index = wrong_idx)
                log.warning(f"Ancestry{idx+1} has {wrong_idx.shape[0]} problematic alleles that couldn't be swapped. Will remove these SNPs.")
            # drop unused columns
            snps = snps.drop(columns = [f"a0_{idx+1}", f"a1_{idx+1}"])
        # rename columns for better indexing in the future
    snps = snps.rename(columns={"a0_1":"a0", "a1_1":"a1"})
    snps.reset_index(inplace=True)
    snps = snps.rename(columns = {"index":"SNPIndex"})

    # find common individuals across geno, pheno, and covar within an ancestry
    common_fam = []
    for idx in range(n_pop):
        # match fam id and pheno id
        tmp_common_fam = pd.merge(fam[idx], pheno[idx][[f"phenoIDX_{idx+1}", "fid", "iid"]], how="inner", on=["fid", "iid"])
        if covar[idx] is not None:
            # match fam id and covar id
            tmp_common_fam = pd.merge(tmp_common_fam, covar[idx][[f"covarIDX_{idx+1}", "fid", "iid"]], how="inner", on=["fid", "iid"])
        common_fam.append(tmp_common_fam)
        common_ind = tmp_common_fam.shape[0]
        if common_ind == 0:
            raise ValueError(f"Ancestry {idx+1}: No common individuals between phenotype and genotype found. Please double check source data.")
        else:
            # sanity check how many we lose
            fam_drop_ind = fam[idx].shape[0] - common_ind
            pheno_drop_ind = pheno[idx].shape[0] - common_ind
            covar_drop_ind = 0
            if covar[idx] is not None:
                covar_drop_ind = covar[idx].shape[0] - common_ind

            if (fam_drop_ind != 0 or pheno_drop_ind != 0) or covar_drop_ind != 0:
                log.info(f"Ancestry {idx+1}: Found {common_ind} common individuals between pehnotype and genotype.")
                log.warning(f"Ancestry {idx+1}: Delete {fam_drop_ind} from fam file. ")
                log.warning(f"Ancestry {idx+1}: Delete {pheno_drop_ind} from phenotype file.")
                log.warning(f"Ancestry {idx+1}: Delete {covar_drop_ind} from covar file.")

    # filter on geno, pheno, and covar
    for idx in range(n_pop):
        # filter on individuals who have both geno, pheno, and covar (if applicable)
        tmp_bed = bed[idx][:, common_fam[idx][f"famIDX_{idx+1}"].values]
        # filter on shared snps across ancestries
        tmp_bed = tmp_bed[snps[f"bimIDX_{idx+1}"].values, :]
        snps = snps.drop(columns = [f"bimIDX_{idx+1}"])
        # flip genotypes for bed files starting second ancestry
        if idx > 0:
            tmp_bed[flip_idx[idx-1]] = 2 - tmp_bed[flip_idx[idx-1]]
        tmp_bed = tmp_bed.T
        # standardize genotype data
        tmp_bed -= jnp.mean(tmp_bed, axis=0)
        tmp_bed /= jnp.std(tmp_bed, axis=0)
        bed[idx] = tmp_bed

        # swap pheno and covar rows order to match fam/bed file, and then select the values for future fine-mapping
        pheno[idx] = pheno[idx].iloc[common_fam[idx][f"phenoIDX_{idx+1}"].values].iloc[:,3].values
        if covar[idx] is not None:
            covar[idx] = covar[idx].iloc[common_fam[idx][f"covarIDX_{idx+1}"].values].iloc[:, 3:(covar[idx].shape[1])].values
        # do we want to add an option to standardize y?

    # estimate heritability
    est_h2g =[]
    for idx in range(n_pop):
        tmp_h2g = estimate_her(bed[idx], pheno[idx], covar[idx])
        est_h2g.append(tmp_h2g)

    # regress covar on y
    for idx in range(n_pop):
        if covar[idx] is not None:
            pheno[idx] = regress_resid(covar[idx], pheno[idx])
            # regress covar on each SNP, it might be slow, the dafault is False
            if resid:
                tmp_n, tmp_p = bed[idx].shape
                for snp in range(tmp_p):
                    # seems jnp array doesn't work in sm, so use np.array
                    bed[idx] = bed[idx].at[:,snp].set(regress_resid(covar[idx], np.array(bed[idx][:,snp])))
    log.info(f"Finished data cleaning. Will fine-map on {bed[0].shape[1]} SNPs.")

    return bed, pheno, covar, snps, est_h2g


def run_finemap(args):
    log = logging.getLogger(sushie.LOG)

    try:
        geno_paths = args.geno.split(":")
        pheno_paths = args.pheno.split(":")

        if len(geno_paths) == len(pheno_paths):
            n_pop = len(geno_paths)
            log.info(f"Detecting {n_pop} features for SuShiE fine-mapping.")
        else:
            raise ValueError("The number of geno and pheno data does not match. Check your input.")

        if args.covar is not None:
            covar_paths = args.covar.split(":")
            if len(covar_paths) != n_pop:
                raise ValueError("The number of covariate data does not match geno data. Check your input.")
        else:
            covar_paths = None

        if args.resid_prior is not None:
            resid_prior = args.resid_prior.split(":")
            if len(resid_prior) != n_pop:
                raise ValueError("The number of specified residual prior does not match feature number. Check your input.")
            resid_prior = [float(i) for i in resid_prior]
            if jnp.any(jnp.array(resid_prior) <= 0):
                raise ValueError(f"The input of residual prior is invalid (<0). Check your input.")
        else:
            resid_prior = None

        if args.effect_prior is not None:
            effect_prior = args.effect_prior.split(":")
            if len(effect_prior ) != n_pop:
                raise ValueError("The number of specified effect prior does not match feature number. Check your input.")
            effect_prior = [float(i) for i in effect_prior]
            if jnp.any(jnp.array(effect_prior) <= 0):
                raise ValueError(f"The input of effect size prior is invalid (<0). Check your input.")
        else:
            effect_prior = None

        if n_pop != 1 and args.rho is not None:
            rho = args.rho.split(":")
            exp_num_rho = math.comb(n_pop, 2)
            if len(rho) != exp_num_rho:
                raise ValueError(f"The number of specified rho ({len(rho)}) does not match expected number {exp_num_rho}. Check your input.")
            rho = [float(i) for i in rho]
            # double check the if it's invalid rho
            if jnp.any(jnp.abs(jnp.array(rho)) >= 1):
                raise ValueError(f"The input of rho is invalid (>=1 or <=-1). Check your input.")
        else:
            rho = None

        # check if the parameters are valid
        if args.pi is not None and (args.pi >= 1 or args.pi <= 0):
            raise ValueError("Pi prior is not a probability (0-1). Specify a valid pi prior.")

        if args.L <= 0:
            raise ValueError("Inferred L is invalid, choose a positive L.")
        elif args.L > 20:
            log.warning(f"The number of inferred L is too large. It may be impractical, and inference may be slow.")

        if args.max_iter < 50:
            log.warning(f"Maximum iteration is low. Inference may not be accurate, try a larger value.")

        if args.min_tol > 0.1:
            log.warning(f"Minimum intolerance is low. Inference may not be accurate, try a smaller value.")

        if args.cs_threshold > 0 and args.cs_threshold < 1:
            cs_threshold = args.cs_threshold
        else:
            raise ValueError("CS threhold is not between 0 and 1. Specify a valid one.")

        if args.purity > 0 and args.purity < 1:
            purity = args.purity
        else:
            raise ValueError("Purity is not between 0 and 1. Specify a valid one.")

        geno_data, pheno_data, covar_data, snps, h2g = process_raw(geno_paths, pheno_paths, covar_paths, args.L, args.resid)
        log.info(f"Successfully prepare genotype and phenotype data for {n_pop} ancestrys, and start fine-mapping using SuShiE.")

        if args.opt_mode == "noop":
            log.warning(f"No updates on the effect size prior because of 'noop' setting for opt_mode. Inference may not be accurate.")

        result = sushie.run_sushie(geno_data, pheno_data, L = args.L, pi = args.pi, resid_var = resid_prior,
                                   effect_var = effect_prior, rho = rho, max_iter = args.max_iter,
                                   min_tol = args.min_tol, opt_mode = args.opt_mode, threshold = cs_threshold,
                                   purity = purity)

        # output credible set
        cs = pd.merge(snps, result.cs, how="inner", on=["SNPIndex"]).drop(columns = ["SNPIndex"]).assign(trait = args.trait).sort_values(by = ["CSIndex", "pip", "cpip"], ascending = [True, False, True])
        cs.to_csv(f"{args.output}.cs.tsv", sep = "\t", index=None)

        # output heritability
        est_her = pd.DataFrame(data = h2g, index = [f"feature{idx+1}" for idx in range(n_pop)], columns = ["h2g"])
        shared_h2g = []
        # only output h2g that has credible sets
        SNPIndex = result.cs.SNPIndex.values.astype(int)
        for idx in range(n_pop):
            tmp_shared_h2g = None
            if result.cs.shape[0] != 0:
                tmp_shared_h2g = estimate_her(geno_data[idx][:, SNPIndex], pheno_data[idx], covar_data[idx])
            shared_h2g.append(tmp_shared_h2g)
        shared_pd = pd.DataFrame(data = shared_h2g, index = [f"feature{idx+1}" for idx in range(n_pop)], columns = ["shared_h2g"])
        est_her = pd.concat([est_her, shared_pd], axis=1).assign(trait = args.trait).reset_index()
        est_her.to_csv(f"{args.output}.h2g.tsv", sep = "\t", index=None)

        # output weights
        snpcopy = copy.deepcopy(snps).assign(trait = args.trait)
        tmp_weights = pd.DataFrame(data = jnp.sum(result.b, axis=0), columns = [f"feature{idx+1}_sushie" for idx in range(n_pop)])
        weights = pd.concat([snpcopy, tmp_weights], axis=1)
        weights.to_csv(f"{args.output}.weights.tsv", sep = "\t", index=None)

        # output variance, covaraince, correlation results
        CSIndex = jnp.unique(result.cs.CSIndex.values.astype(int))
        # only output after-purity CS
        corr_cs_only = jnp.transpose(result.prior_covar_b[CSIndex-1])
        corr = pd.DataFrame(data = {"trait": args.trait, "CSIndex": CSIndex})
        for idx in range(n_pop):
            _var = corr_cs_only[idx, idx]
            tmp_pd = pd.DataFrame(data = {f"feature{idx+1}_est_var": _var})
            corr = pd.concat([corr, tmp_pd], axis=1)
            for jdx in range(idx+1, n_pop):
                _covar = corr_cs_only[idx, jdx]
                _var1 = corr_cs_only[idx, idx]
                _var2 = corr_cs_only[jdx, jdx]
                _corr = _covar / jnp.sqrt(_var1 * _var2)
                tmp_pd_covar = pd.DataFrame(data = {f"feature{idx+1}_feature{jdx+1}_est_covar": _covar})
                tmp_pd_corr = pd.DataFrame(data = {f"feature{idx+1}_feature{jdx+1}_est_corr": _corr})
                corr = pd.concat([corr, tmp_pd_covar, tmp_pd_corr], axis=1)

        corr.to_csv(f"{args.output}.corr.tsv", sep = "\t", index=None)

        # output cross validation results if allowed
        if args.crossval:
            if args.crossval_num <= 1:
                raise ValueError(f"The number of folds in cross validation is invalid. Choose some number greater than 1.")
            elif args.crossval_num > 10:
                log.warning(f"The number of folds in corss validation is too large. Calculation may be slow.")

            rng_key = random.PRNGKey(int(args.seed))
            cv_geno = copy.deepcopy(geno_data)
            cv_pheno = copy.deepcopy(pheno_data)
            if args.crossval_num < 2:
                log.warning(f"Please check cross validation value.")

            log.info(f"Start to create FUSION file.")
            # shuffle the data first
            for idx in range(n_pop):
                rng_key, c_key = random.split(rng_key, 2)
                tmp_n = cv_geno[idx].shape[0]
                ran_index = random.choice(c_key, tmp_n, (tmp_n,), replace=False)
                cv_pheno[idx] = cv_pheno[idx][ran_index]
                cv_geno[idx] = cv_geno[idx][ran_index]

            # create a list to store future estatimated y value
            est_y = [jnp.array([])] * n_pop
            for cv in range(args.crossval_num):
                test_X = []
                train_X = []
                train_y = []
                # make the traing and test for each population separately because sampel size may be different
                for idx in range(n_pop):
                    tmp_n = cv_geno[idx].shape[0]
                    increment = int(tmp_n / args.crossval_num)
                    start = cv * increment
                    end = (cv + 1) * increment
                    # if it is the last fold, take all the rest of the data.
                    if cv == args.crossval_num - 1:
                        end = max(tmp_n, (cv + 1) * increment)
                    test_X.append(cv_geno[idx][start:end])
                    train_X.append(cv_geno[idx][jnp.r_[:start,end:tmp_n]])
                    train_y.append(cv_pheno[idx][jnp.r_[:start,end:tmp_n]])

                log.info(f"Run SuShiE on fold {cv+1}.")
                cv_result = sushie.run_sushie(train_X, train_y, L = args.L, pi = args.pi, resid_var = resid_prior,
                                           effect_var = effect_prior, rho = rho, max_iter = args.max_iter,
                                           min_tol = args.min_tol, opt_mode = args.opt_mode, threshold = cs_threshold,
                                           purity = purity)
                for idx in range(n_pop):
                    tmp_cv_weight = jnp.sum(cv_result.b, axis=0)[:,idx]
                    est_y[idx] = jnp.append(est_y[idx], test_X[idx] @ tmp_cv_weight)
            cv_res = []
            for idx in range(n_pop):
                tmp_est_y = sm.add_constant(est_y[idx])
                get_cvr2 = sm.OLS(np.array(cv_pheno[idx]), tmp_est_y).fit()
                cv_res.append([get_cvr2.rsquared_adj, get_cvr2.pvalues[1]])

            sample_size = [i.shape[0] for i in geno_data]
            cv_r2 = pd.DataFrame(data=cv_res, index = [f"feature{idx+1}" for idx in range(n_pop)], columns = ["rsq", "pval"]).reset_index().assign(N = sample_size)
            cv_r2.to_csv(f"{args.output}.cv.r2.tsv", sep = "\t", index=None)


    except Exception as err:
        log.error(err)
    finally:
        log.info("Finished SuShiE fine-mapping. Thanks for using SuShiE, and have a nice day!")

    return 0

def build_finemap_parser(subp):
    # add imputation parser
    finemap = subp.add_parser("finemap", description="Perform fine-map on individual genotype and phenotype data using SuShiE.")

    # main arguments
    finemap.add_argument("geno",
                     help="Genotype data in plink format. Use semicolon to separete ancestries.")

    finemap.add_argument("pheno",
                     help="Phenotype data. It has to be a tsv file and contains at least three columsn where the first two columns are FID and IID in plink format. No headers. Use semicolon to separete ancestries.")

    # fine-map general options
    finemap.add_argument("--covar", default=None,
                     help="Covariates that will be adjusted in the fine-mapping. It has to be a tsv file and contains at least three columsn where the first two columns are FID and IID in plink format. You need to pre-process the character covariates into dummy variables. No headers. Use semicolon to separate ancestries.")

    finemap.add_argument("--resid", default = False,
                     help="Also regress the covariates out of the genotypes. Default is False.")

    finemap.add_argument("--trait", default="trait",
                     help="Trait or tissue name for the phenotype.")

    finemap.add_argument("--L", default=10,type=int,
                     help="Number of shared effects specified.")

    finemap.add_argument("--pi", default=None, type=float,
                     help="Prior probability for SNPs to be causal. Default is 1/p where p is the number of SNPs in the region.")

    finemap.add_argument("--resid_prior", default=None,
                     help="Specify the prior for the residual variance for ancestries. Use semicolon to separete ancestries.")

    finemap.add_argument("--effect_prior", default=None,
                     help="Specify the prior for the causal variance for ancestries. Use semicolon to separete ancestries.")

    finemap.add_argument("--rho", default=None,
                     help="Specify the prior for the effect correlation for ancestries. Use semicolon to separete paires of ancestries. If two ancestries, one number is needed. If three ancestries, three numbers needed, and the order should be (1,2);(1,3);(2,3) and so forth.")

    finemap.add_argument("--max_iter", default=500,
                     help="Maximum iterations for the optimization.")

    finemap.add_argument("--min_tol", default=1e-5,
                     help="Minimum tolerance for the convergence.")

    finemap.add_argument("--opt_mode", choices=["em", "noop"], default="em",
                     help="Optimization method. 'noop' requires specify the priors for --resid_prior, --effect_prior, and --rho.")

    finemap.add_argument("--cs_threshold", default=0.9, type=float,
                     help="Specify the PIP threshold for SNPs to be included in the credible sets.")

    finemap.add_argument("--purity", default=0.5, type=float,
                     help="Specify the purity threshold for credible sets to be output.")

    # finemap.add_argument("--FUSION", default=False, type=bool,
    #                  help="Indicator for whether output FUSION-format prediction weights.")

    finemap.add_argument("--crossval", default=False, type=bool,
                     help="Indicator to output cross validation results for FUSION pipline.")

    finemap.add_argument("--crossval_num", default=5, type=int,
                     help="The number of fold cross validation.")

    finemap.add_argument("--seed", default=12345, type=int,
                     help="The seed to randomly cut data sets for cross validation.")

    # misc options
    finemap.add_argument("--quiet", default=False, action="store_true",
                      help="Do not print anything to stdout.")
    finemap.add_argument("--verbose", default=False, action="store_true",
                      help="Verbose logging. Includes debug info.")
    finemap.add_argument("--output", default="sushie_finemap",
                      help="Prefix for output data.")

    return finemap



def main(argsv):
    # setup main parser
    argp = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    subp = argp.add_subparsers(
        help="Subcommands: finemap to perform genotype fine-mapping using SuShiE, twas to perform transcriptome-wide association studies")

    # add subparsers for focus commands
    finemap = build_finemap_parser(subp)
    finemap.set_defaults(func=run_finemap)

    # parse arguments
    args = argp.parse_args(argsv)

    # hack to check that at least one sub-command was selected in 3.6
    # 3.7 -might- have fixed this bug
    if not hasattr(args, "func"):
        argp.print_help()
        return 2  # command-line error

    cmd_str = get_command_string(argsv)

    masthead =  "===================================" + os.linesep
    masthead += "             SuShiE v{}             ".format(sushie.VERSION) + os.linesep
    masthead += "===================================" + os.linesep

    # setup logging
    log_format = "[%(asctime)s - %(levelname)s] %(message)s"
    date_format = "%Y-%m-%d %H:%M:%S"
    log = logging.getLogger(sushie.LOG)
    if args.verbose:
        log.setLevel(logging.DEBUG)
    else:
        log.setLevel(logging.INFO)
    fmt = logging.Formatter(fmt=log_format, datefmt=date_format)

    # write to stdout unless quiet is set
    if not args.quiet:
        sys.stdout.write(masthead)
        sys.stdout.write(cmd_str)
        sys.stdout.write("Starting log..." + os.linesep)
        stdout_handler = logging.StreamHandler(sys.stdout)
        stdout_handler.setFormatter(fmt)
        log.addHandler(stdout_handler)

    # setup log file, but write PLINK-style command first
    disk_log_stream = open(f"{args.output}.log", "w")
    disk_log_stream.write(masthead)
    disk_log_stream.write(cmd_str)
    disk_log_stream.write("Starting log..." + os.linesep)

    disk_handler = logging.StreamHandler(disk_log_stream)
    disk_handler.setFormatter(fmt)
    log.addHandler(disk_handler)

    # launch finemap
    args.func(args)

    return 0


if __name__ == "__main__":
    sys.exit(main(sys.argv[1:]))
